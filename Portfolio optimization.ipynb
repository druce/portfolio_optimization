{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [README.md](https://github.com/druce/portfolio_optimization/blob/master/README.md) for discussion, environment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "import time \n",
    "import requests\n",
    "import dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import xlrd\n",
    "\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, leaves_list\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import openbb\n",
    "from openbb import obb\n",
    "from openbb_core.app.model.obbject import OBBject\n",
    "\n",
    "# https://www.cvxpy.org/install/index.html\n",
    "import cvxpy as cp\n",
    "\n",
    "# https://riskfolio-lib.readthedocs.io/en/latest/\n",
    "import riskfolio as rp\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(2347)\n",
    "\n",
    "print(\"%-20s %s\" % ('python', \".\".join(map(str, sys.version_info[:3]))))\n",
    "print(\"%-20s %s\" % (\"numpy\", np.__version__))\n",
    "print(\"%-20s %s\" % (\"scipy\", scipy.__version__))\n",
    "\n",
    "print(\"%-20s %s\" % (\"pandas\", pd.__version__))\n",
    "print(\"%-20s %s\" % (\"pandas-datareader\", pdr.__version__))\n",
    "# print(\"%-20s %s\" % (\"xlrd\", xlrd.__version__))\n",
    "print(\"%-20s %s\" % (\"seaborn\", sns.__version__))\n",
    "print(\"%-20s %s\" % (\"matplotlib\", matplotlib.__version__))\n",
    "print(\"%-20s %s\" % (\"cvxpy\", cp.__version__))\n",
    "print(\"%-20s %s\" % (\"openbb\", obb.system.version))\n",
    "\n",
    "print(\"%-20s %s\" % (\"riskfolio\", rp.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spreadsheet from Damodaran website into pandas dataframe\n",
    "\n",
    "# if below gives cert error\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "data_xls = 'https://www.stern.nyu.edu/~adamodar/pc/datasets/histretSP.xls'\n",
    "data_sheet = \"Returns by year\"\n",
    "# these will change as rows get added on Damodaran website\n",
    "skiprows = range(19)\n",
    "skipfooter = 13\n",
    "download_df = pd.read_excel(data_xls, \n",
    "                         sheet_name=data_sheet, \n",
    "                         skiprows=skiprows,\n",
    "                         skipfooter=skipfooter)\n",
    "download_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index to year as int\n",
    "download_df[\"Year\"] = download_df[\"Year\"].astype(int)\n",
    "download_df.set_index(download_df[\"Year\"], inplace=True)\n",
    "download_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download GDP for correlation matrix\n",
    "series = ['GDPCA']\n",
    "\n",
    "gdp_download = pdr.data.DataReader(series, \n",
    "                                   'fred', \n",
    "                                   start='1926-12-31')\n",
    "gdp_download.reset_index(inplace=True)\n",
    "gdp_download.set_index(pd.DatetimeIndex(gdp_download['DATE']).year, inplace=True)\n",
    "gdp_download['GDP'] = gdp_download['GDPCA'].pct_change()\n",
    "# https://fortunly.com/statistics/us-gdp-by-year-guide/#gref\n",
    "gdp_download.loc[1928, 'GDP'] = 0.0110\n",
    "gdp_download.loc[1929, 'GDP'] = 0.0652\n",
    "gdp_download.sort_index(inplace=True)\n",
    "gdp_download.to_csv('gdp_fred.csv')\n",
    "\n",
    "gdp_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_df = download_df.copy()\n",
    "real_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use caution to grab real return columns, not nominal, column names are similar\n",
    "# check values vs. sheet\n",
    "real_data_df = download_df.copy()\n",
    "real_data_df = real_data_df.drop(columns=[\"Real Estate\"])\n",
    "real_data_df = real_data_df.rename(\n",
    "    columns={\n",
    "        \"Real Estate\": \"Real Estate (nominal)\",\n",
    "        'Inflation Rate': 'CPI',\n",
    "        'S&P 500 (includes dividends)2': 'S&P',\n",
    "        \"US Small cap (bottom decile)22\": \"Small Caps\",\n",
    "        '!0-year T.Bonds': 'T-Notes',\n",
    "        '3-month T. Bill (Real)': 'T-Bills',\n",
    "        'Baa Corp Bonds': 'Baa Corps',\n",
    "        'Real Estate3': 'Real Estate',\n",
    "    })\n",
    "\n",
    "real_data_df[\"GDP\"] = gdp_download['GDP']\n",
    "# filter and reorder\n",
    "real_data_df = real_data_df[[\n",
    "    'GDP',\n",
    "    'CPI',\n",
    "    'S&P',\n",
    "    'Small Caps',\n",
    "    'T-Bills',\n",
    "    'T-Notes',\n",
    "    'Baa Corps',\n",
    "    'Real Estate',\n",
    "    'Gold',\n",
    "]]\n",
    "real_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumreturns = (1 + real_data_df.copy()).cumprod()\n",
    "(cumreturns.iloc[-1]-1)**(1/len(cumreturns))-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gold data v MacroTrends\n",
    "# gdf = pd.read_csv('MacroTrends_Data_Download.csv')\n",
    "# gdf[\"Year\"] = pd.to_datetime(gdf['date'])\n",
    "# gdf = gdf.loc[gdf[\"Year\"].dt.month ==12]\n",
    "# gdf[\"Year\"] = gdf[\"Year\"].dt.year\n",
    "# gdf = gdf.set_index(\"Year\", drop=True)\n",
    "# gdf['real_pct'] = gdf[\"real\"].pct_change()\n",
    "# gdf['nom_pct'] = gdf[\"nominal\"].pct_change()\n",
    "# gdf[\"damodaran_real_pct\"]=download_df[\"Gold\"]\n",
    "# gdf[\"damodaran_nom_pct\"]=download_df[\"Gold*\"]\n",
    "# gdf[\"real_diff\"] = gdf[\"damodaran_real_pct\"]-gdf[\"real_pct\"]\n",
    "# gdf[\"nom_diff\"] = gdf[\"damodaran_nom_pct\"]-gdf[\"nom_pct\"]\n",
    "# gdf[[\"nom_diff\", 'real_diff']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # display all rows without truncation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in real_data_df.columns:\n",
    "    real_data_df[col] = real_data_df[col].astype(float)\n",
    "\n",
    "# compute correlation matrix\n",
    "my_cmap = sns.diverging_palette(10, 220, sep=80, n=50)\n",
    "sns.heatmap(real_data_df.corr(), annot=True, fmt=\".02f\", cmap=my_cmap);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop CPI, GDP which are not assets\n",
    "try:\n",
    "    real_data_df.drop(labels=['CPI', 'GDP'], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "df = real_data_df.copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.line();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation matrix\n",
    "my_cmap = sns.diverging_palette(10, 220, sep=80, n=50)\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".02f\", cmap=my_cmap);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot historical cumulative growth\n",
    "df2 = df.copy()\n",
    "for col in df2.columns:\n",
    "    df2[col]+= 1\n",
    "    df2[col] = df2[col].cumprod()\n",
    "    \n",
    "df2.plot.line();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot historical cumulative growth since 1970\n",
    "df2 = df.copy().loc[1970:]\n",
    "for col in df2.columns:\n",
    "    df2[col]+= 1\n",
    "    df2[col] = df2[col].cumprod()\n",
    "    \n",
    "df2.plot.line();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df.columns)\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-only optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1928 - present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arithmetic means\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geometric mean\n",
    "cumreturns = (1 + df.copy()).cumprod()\n",
    "(cumreturns.iloc[-1]-1)**(1/len(cumreturns))-1\n",
    "# difference due to volatility and compounding and maybe divergences from IID log normal distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance matrix\n",
    "Sigma = np.cov(df.transpose())\n",
    "# number of assets\n",
    "\n",
    "n = Sigma.shape[0]\n",
    "# average returns\n",
    "mu = df.mean().values\n",
    "# asset STDs\n",
    "asset_vols = np.sqrt(Sigma.diagonal())\n",
    "# variable to optimize over - portfolio weights\n",
    "w = cp.Variable(n)\n",
    "\n",
    "# objectives to optimize\n",
    "# portfolio return\n",
    "ret = mu.T @ w \n",
    "# volatility\n",
    "vol = cp.quad_form(w, Sigma)\n",
    "\n",
    "z = pd.DataFrame([mu, asset_vols], columns=labels)\n",
    "z['rows'] = ['real return', 'vol']\n",
    "z.set_index('rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve max return portfolio (corner solution)\n",
    "# should be 100% highest return asset\n",
    "prob = cp.Problem(cp.Maximize(ret),      # maximize return\n",
    "                  [cp.sum(w) == 1,       # weights sum to 1\n",
    "                   w >= 0]               # each w > 0\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "maxretvol = vol.value\n",
    "maxret = ret.value\n",
    "print(\"Max return portfolio weights\")\n",
    "pd.DataFrame([wts], columns=labels)\n",
    "# all stocks which is highest return asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve min vol portfolio (other corner solution)\n",
    "# should be mostly T-bills but there is variance in t-bills so it diversifies\n",
    "prob = cp.Problem(cp.Minimize(vol),\n",
    "                  [cp.sum(w) == 1,     # weights sum to 1\n",
    "                   w >= 0],            # each weight >= 0\n",
    "                 )\n",
    "prob.solve()\n",
    "# round to not get x.xxxxE-22\n",
    "wts = [float('%0.6f' % v) for v in w.value]\n",
    "\n",
    "minvol = vol.value\n",
    "minvolret = ret.value\n",
    "print(\"Min vol portfolio weights\")\n",
    "pd.DataFrame([wts], columns=labels)\n",
    "# mostly t-bills and real estate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# solve points in between\n",
    "# for a series of points between min and max vol, maximize return subject to volatility constraints \n",
    "\n",
    "# specify a Parameter variable instead of creating new Problem at each iteration\n",
    "# this allows the solver to reuse previous work\n",
    "vol_limit = cp.Parameter(nonneg=True)\n",
    "\n",
    "prob = cp.Problem(cp.Maximize(ret),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0,\n",
    "                   vol <= vol_limit\n",
    "                  ]\n",
    "                 )\n",
    "\n",
    "# define function so we can solve many in parallel\n",
    "def solve_vl(vl_val):\n",
    "    vol_limit.value = vl_val\n",
    "    result = prob.solve()\n",
    "    return (ret.value, np.sqrt(vol.value), w.value)\n",
    "\n",
    "# number of points on the frontier\n",
    "NPOINTS = 200\n",
    "vl_vals = np.linspace(np.sqrt(minvol), np.sqrt(maxretvol), NPOINTS)\n",
    "vl_vals = np.square(vl_vals)\n",
    "# vol constraint is in variance space, take square root of minvol and maxvol, linspace, square values)\n",
    "\n",
    "# iterate in-process\n",
    "results_dict = {}\n",
    "for vl_val in vl_vals:\n",
    "    # print(datetime.strftime(datetime.now(), \"%H:%M:%S\"), vl_val)\n",
    "    results_dict[vl_val] = solve_vl(vl_val)\n",
    "    \n",
    "# parallel implementation\n",
    "# NPROCESSES = 8\n",
    "# pool = Pool(processes = NPROCESSES)\n",
    "# result_values = pool.map(solve_vl, vl_vals)\n",
    "# results_dict = dict(zip(vl_vals, result_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df = pd.DataFrame(enumerate(results_dict.keys()))\n",
    "ret_df.columns=['i', 'vol']\n",
    "ret_df['return'] = [results_dict[v][0] for v in ret_df['vol']]\n",
    "ret_df['std'] = [results_dict[v][1] for v in ret_df['vol']]\n",
    "for i, colname in enumerate(labels):\n",
    "    ret_df[colname]=[results_dict[v][2][i] for v in ret_df['vol']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot efficient frontier\n",
    "def plot_efrontier(ret_df, df,\n",
    "                   xlabel=\"Standard Deviation of Real Returns\",\n",
    "                   ylabel=\"Real Return\",\n",
    "                   title=None):\n",
    "\n",
    "    Sigma = np.cov(df.transpose())\n",
    "    n = Sigma.shape[0]\n",
    "    mu = df.mean().values\n",
    "    asset_vols = np.sqrt(Sigma.diagonal())\n",
    "\n",
    "    risk_free_rate = 0  # availability of any risk-free real rate in this context is debatable \n",
    "    ret_df[\"Sharpe\"] = (ret_df[\"return\"] - risk_free_rate) / ret_df[\"std\"]\n",
    "    \n",
    "    max_sharpe_index = ret_df[\"Sharpe\"].argmax()  \n",
    "#     print(max_sharpe_index)\n",
    "    max_sharpe_return = ret_df.iloc[max_sharpe_index][\"return\"]\n",
    "#     print(max_sharpe_return)\n",
    "    max_sharpe_std = ret_df.iloc[max_sharpe_index][\"std\"]\n",
    "#     print(max_sharpe_std)\n",
    "    max_sharpe_ratio = ret_df.iloc[max_sharpe_index][\"Sharpe\"]\n",
    "    \n",
    "    asset_names = [t for t in ['TIPS', 'T-Bills', 'Real Estate', 'T-Notes', 'Baa Corps', 'Gold', 'S&P', 'Small Caps', 'shorts'] if t in ret_df.columns]\n",
    "    \n",
    "    mean_wts = ret_df[asset_names].mean() # average weights over all efficient portolios\n",
    "    temp_ret_df = df[asset_names]         # historical returns\n",
    "    avg_ret = temp_ret_df @ mean_wts.values\n",
    "    avg_ret_mean = avg_ret.mean()\n",
    "    avg_ret_std = avg_ret.std()\n",
    "    \n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "\n",
    "    # plot the data\n",
    "    plt.plot(ret_df['std'], ret_df['return'])\n",
    "    # Force both axes to start at 0\n",
    "    plt.xlim(left=0, right=max(asset_vols))\n",
    "    plt.ylim(bottom=min(0, min(mu)))\n",
    "    \n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plot_title = \"Risk vs. Real Return,  %d-%d\" % (df.index[0], df.index[-1]) if title is None else title\n",
    "    plt.title(plot_title)\n",
    "\n",
    "    # plot the markers\n",
    "    plt.scatter(asset_vols, mu)\n",
    "    xoffset = 0.0025\n",
    "    yoffset = 0.0015\n",
    "    labels = df.columns\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.annotate(label, xy=(asset_vols[i]+xoffset, mu[i]+yoffset), xycoords='data',\n",
    "                     horizontalalignment='left', verticalalignment='top',\n",
    "                    )\n",
    "    plt.scatter([max_sharpe_std], [max_sharpe_return])\n",
    "    plt.annotate(\"Max Sharpe\", xy=(max_sharpe_std+xoffset, max_sharpe_return+yoffset), xycoords='data',\n",
    "                 horizontalalignment='left', verticalalignment='top',\n",
    "                )\n",
    "    plt.scatter([avg_ret_std], [avg_ret_mean])\n",
    "    plt.annotate(\"EF Avg Wts\", xy=(avg_ret_std+xoffset, avg_ret_mean+yoffset), xycoords='data',\n",
    "                 horizontalalignment='left', verticalalignment='top',\n",
    "                )\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    print(\"Max Sharpe Portfolio:\")\n",
    "    print(f\"Real Return:  {100*max_sharpe_return:3.2f}%\")\n",
    "    print(f\"SD:           {100*max_sharpe_std:3.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {max_sharpe_ratio:3.3f}\")\n",
    "\n",
    "    for col in asset_names:\n",
    "        print(f\"{col}: {100*ret_df.iloc[max_sharpe_index][col]:3.1f}%\")\n",
    "  \n",
    "    print()\n",
    "    print(\"Average over efficient frontier:\")\n",
    "    print(f\"Real Return:  {100*avg_ret_mean:3.2f}%\")\n",
    "    print(f\"SD:           {100*avg_ret_std:3.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {avg_ret_mean/avg_ret_std:3.3f}\")\n",
    "    for col in asset_names:\n",
    "        print(f\"{col}: {100*mean_wts[col]:3.1f}%\")\n",
    "            \n",
    "    return max_sharpe_return, max_sharpe_std, avg_ret_mean, avg_ret_std\n",
    "        \n",
    "max_sharpe_return, max_sharpe_std, avg_ret_mean, avg_ret_std = plot_efrontier(ret_df, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked area chart of weights vs. returns\n",
    "# for given vol constraint and corresponding real return, show portfolio weights\n",
    "def transition_map(ret_df, labels, startyear, endyear, max_sharpe_return=None, avg_ret_mean=None, ylim=1):\n",
    "    \n",
    "    x = ret_df['return']\n",
    "    # absolute values so shorts don't create chaos\n",
    "    y_list = [abs(ret_df[l]) for l in labels]\n",
    "    pal = ['red', 'lightgreen', 'darkgreen', 'navy', 'cyan', 'violet', 'gold', ]\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 4.5))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "  \n",
    "    ax1.stackplot(x, y_list, labels=labels, colors=pal)\n",
    "    ax1.set_xlim((ret_df['return'].iloc[0], ret_df['return'].iloc[-1]))\n",
    "    ax1.set_ylim((0, ylim))\n",
    "    ax1.set_xlabel('Portfolio Vol')\n",
    "    ax1.set_xlabel(\"Portfolio Real Return\")\n",
    "    ax1.set_ylabel(\"Portfolio Weight\")\n",
    "    ax1.legend(loc='lower right')\n",
    "#     return/std relationship is not linear, can't have both axes\n",
    "#     ax2 = ax1.twiny()\n",
    "#     ax2.set_xlim((ret_df['std'].iloc[0], ret_df['std'].iloc[-1]))\n",
    "#     ax2.set_xlabel('Portfolio Vol')\n",
    "    \n",
    "    if max_sharpe_return is not None:\n",
    "        ax1.axvline(max_sharpe_return, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# don't draw line for avg_ret_mean, doesn't correspond to a portfolio in the transition map\n",
    "#     if avg_ret_mean is not None:\n",
    "#         ax1.axvline(avg_ret_mean, color='black', linestyle='--', linewidth=1)\n",
    "        \n",
    "    plt.title(\"Optimal Portfolio Transition Map, %d-%d\" % (startyear, endyear), y=1.16);\n",
    "\n",
    "transition_map(ret_df, labels=df.columns, startyear=df.index[0], endyear=df.index[-1], max_sharpe_return=max_sharpe_return, avg_ret_mean=avg_ret_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1967 - present (more inflationary era including post gold standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = real_data_df.loc[1967:]\n",
    "df.plot.line();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance matrix\n",
    "Sigma = np.cov(df.transpose())\n",
    "# number of assets\n",
    "\n",
    "n = Sigma.shape[0]\n",
    "# average returns\n",
    "mu = df.mean().values\n",
    "# asset STDs\n",
    "asset_vols = np.sqrt(Sigma.diagonal())\n",
    "# variable to optimize over - portfolio weights\n",
    "w = cp.Variable(n)\n",
    "\n",
    "# objectives to optimize\n",
    "# portfolio return\n",
    "ret = mu.T @ w \n",
    "# volatility\n",
    "vol = cp.quad_form(w, Sigma)\n",
    "\n",
    "z = pd.DataFrame([mu, asset_vols], columns=labels)\n",
    "z['rows'] = ['real return', 'vol']\n",
    "z.set_index('rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve max return portfolio (corner solution)\n",
    "prob = cp.Problem(cp.Maximize(ret), \n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0]\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "maxretvol = vol.value\n",
    "maxret = ret.value\n",
    "print(\"Max return portfolio weights\")\n",
    "pd.DataFrame([wts], columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve min vol portfolio (other corner solution)\n",
    "prob = cp.Problem(cp.Minimize(vol),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0]\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "\n",
    "minvol = vol.value\n",
    "minvolret = ret.value\n",
    "print(\"Min vol portfolio weights\")\n",
    "pd.DataFrame([wts], columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# solve points in between\n",
    "# maximize return subject to volatility constraints between minimum volatility and max return volatility\n",
    "\n",
    "# specify a Parameter variable instead of creating new Problem at each iteration\n",
    "# this allows the solver to reuse previous work\n",
    "vol_limit = cp.Parameter(nonneg=True)\n",
    "\n",
    "prob = cp.Problem(cp.Maximize(ret),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0,\n",
    "                   vol <= vol_limit\n",
    "                  ]\n",
    "                 )\n",
    "\n",
    "# define function so we can solve many in parallel\n",
    "def solve_vl(vl_val):\n",
    "    vol_limit.value = vl_val\n",
    "    result = prob.solve()\n",
    "    return (ret.value, np.sqrt(vol.value), w.value)\n",
    "\n",
    "# number of points on the frontier\n",
    "NPOINTS = 200\n",
    "vl_vals = np.linspace(np.sqrt(minvol), np.sqrt(maxretvol), NPOINTS)\n",
    "vl_vals = np.square(vl_vals)\n",
    "# vol constraint is in variance space, take square root of minvol and maxvol, linspace, square values)\n",
    "\n",
    "# iterate in-process\n",
    "results_dict = {}\n",
    "for vl_val in vl_vals:\n",
    "    # print(datetime.strftime(datetime.now(), \"%H:%M:%S\"), vl_val)\n",
    "    results_dict[vl_val] = solve_vl(vl_val)\n",
    "    \n",
    "# parallel implementation\n",
    "# NPROCESSES = 8\n",
    "# pool = Pool(processes = NPROCESSES)\n",
    "# result_values = pool.map(solve_vl, vl_vals)\n",
    "# results_dict = dict(zip(vl_vals, result_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df = pd.DataFrame(enumerate(results_dict.keys()))\n",
    "ret_df.columns=['i', 'vol']\n",
    "ret_df['return'] = [results_dict[v][0] for v in ret_df['vol']]\n",
    "ret_df['std'] = [results_dict[v][1] for v in ret_df['vol']]\n",
    "for i, colname in enumerate(labels):\n",
    "    ret_df[colname]=[results_dict[v][2][i] for v in ret_df['vol']]\n",
    "# ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sharpe_return, max_sharpe_std, avg_ret_mean, avg_ret_std = plot_efrontier(ret_df, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_map(ret_df, labels=df.columns, startyear=df.index[0], endyear=df.index[-1], max_sharpe_return=max_sharpe_return, avg_ret_mean=avg_ret_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1983 - present (era of globalization, post-big inflation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = real_data_df.loc[1983:]\n",
    "df.plot.line();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance matrix\n",
    "Sigma = np.cov(df.transpose())\n",
    "# number of assets\n",
    "\n",
    "n = Sigma.shape[0]\n",
    "# average returns\n",
    "mu = df.mean().values\n",
    "# asset STDs\n",
    "asset_vols = np.sqrt(Sigma.diagonal())\n",
    "# variable to optimize over - portfolio weights\n",
    "w = cp.Variable(n)\n",
    "\n",
    "# objectives to optimize\n",
    "# portfolio return\n",
    "ret = mu.T @ w \n",
    "# volatility\n",
    "vol = cp.quad_form(w, Sigma)\n",
    "\n",
    "z = pd.DataFrame([mu, asset_vols], columns=labels)\n",
    "z['rows'] = ['real return', 'vol']\n",
    "z.set_index('rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solve max return portfolio (corner solution)\n",
    "prob = cp.Problem(cp.Maximize(ret), \n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0]\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "maxretvol = vol.value\n",
    "maxret = ret.value\n",
    "print(\"Max return portfolio weights\")\n",
    "pd.DataFrame([wts], columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve min vol portfolio (other corner solution)\n",
    "prob = cp.Problem(cp.Minimize(vol),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0]\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "\n",
    "minvol = vol.value\n",
    "minvolret = ret.value\n",
    "print(\"Min vol portfolio weights\")\n",
    "pd.DataFrame([wts], columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# solve points in between\n",
    "# maximize return subject to volatility constraints between minimum volatility and max return volatility\n",
    "\n",
    "# specify a Parameter variable instead of creating new Problem at each iteration\n",
    "# this allows the solver to reuse previous work\n",
    "vol_limit = cp.Parameter(nonneg=True)\n",
    "\n",
    "prob = cp.Problem(cp.Maximize(ret),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0,\n",
    "                   vol <= vol_limit\n",
    "                  ]\n",
    "                 )\n",
    "\n",
    "# define function so we can solve many in parallel\n",
    "def solve_vl(vl_val):\n",
    "    vol_limit.value = vl_val\n",
    "    result = prob.solve()\n",
    "    return (ret.value, np.sqrt(vol.value), w.value)\n",
    "\n",
    "# number of points on the frontier\n",
    "NPOINTS = 200\n",
    "vl_vals = np.linspace(np.sqrt(minvol), np.sqrt(maxretvol), NPOINTS)\n",
    "vl_vals = np.square(vl_vals)\n",
    "# vol constraint is in variance space, take square root of minvol and maxvol, linspace, square values)\n",
    "\n",
    "# iterate in-process\n",
    "results_dict = {}\n",
    "for vl_val in vl_vals:\n",
    "    # print(datetime.strftime(datetime.now(), \"%H:%M:%S\"), vl_val)\n",
    "    results_dict[vl_val] = solve_vl(vl_val)\n",
    "    \n",
    "# parallel implementation\n",
    "# NPROCESSES = 8\n",
    "# pool = Pool(processes = NPROCESSES)\n",
    "# result_values = pool.map(solve_vl, vl_vals)\n",
    "# results_dict = dict(zip(vl_vals, result_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ret_df = pd.DataFrame(enumerate(results_dict.keys()))\n",
    "ret_df.columns=['i', 'vol']\n",
    "ret_df['return'] = [results_dict[v][0] for v in ret_df['vol']]\n",
    "ret_df['std'] = [results_dict[v][1] for v in ret_df['vol']]\n",
    "for i, colname in enumerate(labels):\n",
    "    ret_df[colname]=[results_dict[v][2][i] for v in ret_df['vol']]\n",
    "# ret_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sharpe_return, max_sharpe_std, avg_ret_mean, avg_ret_std = plot_efrontier(ret_df, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_map(ret_df, labels=df.columns, startyear=df.index[0], endyear=df.index[-1], max_sharpe_return=max_sharpe_return, avg_ret_mean=avg_ret_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a risk-free asset\n",
    "- The efficient frontier above does not include a risk-free asset, when we inflation-adjust t-bill returns we get volatility and fluctuation in returns including periods of negative real returns.\n",
    "- However TIPS are available which offer a guaranteed real pre-tax return. They are issued at a real rate, the principal gets adjusted for inflation, and if there is deflation you can't get back less than par. So when you buy TIPS you are guaranteed a positive real pre-tax return\n",
    "- TIPS offer an inflation hedge and a safe real return, so they might dominate gold. There isn't a great theoretical argument gold should increase in value faster than inflation in the long run (gold bugs might disagree but in a fiat world, that's my story and I'm sticking to it). I could see reasonable arguments why gold should maintain its real value if supply is fixed, and there should be demand for gold when there is inflation and people lose faith in monetary authorities because it is currency-like and supply is relatively fixed, so gold offers an inflation hedge. \n",
    "- TIPS total returns are only available for approximately the last 25 years. You can model the TIPS yield as the yield on similar nominal Treasuries less inflation expectations. Hypothetically, there might be a sound way to model historical inflation expectations using recent inflation trends, gold, steepness of yield curve etc. And from there, model what TIPS total returns would theoretically have been based on Treasury total returns and changes in inflation expectations, but that is a challenge. We could also say that the best inflation hedge was gold up to 2000 and TIPS thereafter and use VIPSX OR TIP, but that is a kinky Franken-asset.\n",
    "- We could also say that given the existence of TIPS, a risk-free 0 real yield asset is available. Worst case TIPS return is 0, if auction rate is 0. Or you could buy TIPS and donate any return over 0, and you are guaranteed return of principal plus inflation. You could argue that it wasn't available and if it had been then it would have modified other returns. If my aunt had wheels she'd be a bicycle.\n",
    "- Lets posit that we are justified in adding a risk-free TIPS asset, with a constant zero return.\n",
    "- In the real world you would get a positive real return on TIPS with some fluctuations, real TIPS should dominate the risk-free asset. So this model might underweight TIPS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = real_data_df.loc[1928:].copy()\n",
    "df[\"TIPS\"] = 0\n",
    "# reorder  for chart\n",
    "df = df[[ 'S&P', 'Small Caps', 'T-Notes', 'Baa Corps', 'TIPS', 'Real Estate', 'Gold', 'T-Bills' ]]\n",
    "labels = df.columns\n",
    "df.plot.line();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance matrix\n",
    "Sigma = np.cov(df.transpose())\n",
    "\n",
    "# number of assets\n",
    "n = Sigma.shape[0]\n",
    "# average returns\n",
    "mu = df.mean().values\n",
    "# asset STDs\n",
    "asset_vols = np.sqrt(Sigma.diagonal())\n",
    "# variable to optimize over - portfolio weights\n",
    "w = cp.Variable(n)\n",
    "\n",
    "# objectives to optimize\n",
    "# portfolio return\n",
    "ret = mu.T @ w \n",
    "\n",
    "# volatility\n",
    "vol = cp.quad_form(w, Sigma)\n",
    "\n",
    "z = pd.DataFrame([mu, asset_vols], columns=labels)\n",
    "z['rows'] = ['real return', 'vol']\n",
    "z.set_index('rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve max return portfolio (corner solution)\n",
    "prob = cp.Problem(cp.Maximize(ret), \n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0]\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "maxretvol = vol.value\n",
    "maxret = ret.value\n",
    "print(\"Max return portfolio weights\")\n",
    "pd.DataFrame([wts], columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve min vol portfolio (other corner solution)\n",
    "prob = cp.Problem(cp.Minimize(vol),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0]\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "\n",
    "minvol = vol.value\n",
    "minvolret = ret.value\n",
    "print(\"Min vol portfolio weights\")\n",
    "pd.DataFrame([wts], columns=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# solve points in between\n",
    "# maximize return subject to volatility constraints between minimum volatility and max return volatility\n",
    "\n",
    "# specify a Parameter variable instead of creating new Problem at each iteration\n",
    "# this allows the solver to reuse previous work\n",
    "vol_limit = cp.Parameter(nonneg=True)\n",
    "\n",
    "prob = cp.Problem(cp.Maximize(ret),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   w >= 0,\n",
    "                   vol <= vol_limit\n",
    "                  ]\n",
    "                 )\n",
    "\n",
    "# define function so we can solve many in parallel\n",
    "def solve_vl(vl_val):\n",
    "    vol_limit.value = vl_val\n",
    "    result = prob.solve()\n",
    "    return (ret.value, np.sqrt(vol.value), w.value)\n",
    "\n",
    "# number of points on the frontier\n",
    "NPOINTS = 200\n",
    "vl_vals = np.linspace(np.sqrt(minvol), np.sqrt(maxretvol), NPOINTS)\n",
    "vl_vals = np.square(vl_vals)\n",
    "# vol constraint is in variance space, take square root of minvol and maxvol, linspace, square values)\n",
    "\n",
    "# iterate in-process\n",
    "results_dict = {}\n",
    "for vl_val in vl_vals:\n",
    "    # print(datetime.strftime(datetime.now(), \"%H:%M:%S\"), vl_val)\n",
    "    results_dict[vl_val] = solve_vl(vl_val)\n",
    "    \n",
    "# parallel implementation\n",
    "# NPROCESSES = 8\n",
    "# pool = Pool(processes = NPROCESSES)\n",
    "# result_values = pool.map(solve_vl, vl_vals)\n",
    "# results_dict = dict(zip(vl_vals, result_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ret_df = pd.DataFrame(enumerate(results_dict.keys()))\n",
    "ret_df.columns=['i', 'vol']\n",
    "ret_df['return'] = [results_dict[v][0] for v in ret_df['vol']]\n",
    "ret_df['std'] = [results_dict[v][1] for v in ret_df['vol']]\n",
    "for i, colname in enumerate(labels):\n",
    "    ret_df[colname]=[results_dict[v][2][i] for v in ret_df['vol']]\n",
    "# ret_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sharpe_return, max_sharpe_std, avg_ret_mean, avg_ret_std = plot_efrontier(ret_df, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_map(ret_df, labels=df.columns, startyear=df.index[0], endyear=df.index[-1], max_sharpe_return=max_sharpe_return, avg_ret_mean=avg_ret_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midwit regularization - take the mean of all optimal portfolios at any level of risk\n",
    "regularized = ret_df[['S&P', 'Small Caps', 'T-Notes',\n",
    "       'Baa Corps', 'TIPS', 'Real Estate', 'Gold', 'T-Bills']].mean()\n",
    "with pd.option_context('display.float_format', '{:.6f}'.format):\n",
    "    display(regularized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long/short optimization with leverage constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = real_data_df['S&P'].values\n",
    "n = len(x1)\n",
    "target_corr = 0.6\n",
    "theta = np.arccos(target_corr)\n",
    "mean_return = -0.05\n",
    "x2 = np.random.normal(0, x1.std(), size=n)  \n",
    "# center so actual mean = 0\n",
    "X = pd.DataFrame({'x1': x1 - x1.mean(),\n",
    "                  'x2': x2 - x2.mean()\n",
    "                 })\n",
    "# identity matrix\n",
    "Id = np.diag(np.ones(n))\n",
    "# QR factorization \n",
    "Q, R = np.linalg.qr(X[['x1']])\n",
    "P = Q @ Q.T\n",
    "x2o = (Id - P) @ X[['x2']]\n",
    "Xc2 = pd.DataFrame({'x1': X['x1'], 'x2': x2o['x2']})\n",
    "# divide by l2 norm\n",
    "Y = Xc2 / np.sqrt(np.sum(np.square(Xc2), axis=0))\n",
    "retval = Y['x2'] + (1/np.tan(theta)) * Y['x1'] + mean_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to include shorts, we need an asset with a poor expected return\n",
    "# assume I can identify bad stocks, they are highly correlated with S&P but return negative 5%\n",
    "# create a synthetic 'shorts' asset \n",
    "\n",
    "n = len(real_data_df)\n",
    "target_corr = 0.6\n",
    "def target_corr(x1, target_corr, mean_return):\n",
    "    \"\"\"given a series x1, return a random series with correlation target_corr to x1\"\"\"\n",
    "    n = len(x1)\n",
    "    theta = np.arccos(target_corr)\n",
    "    \n",
    "    x2 = np.random.normal(0, x1.std(), size=n)  \n",
    "    # center so actual mean = 0\n",
    "    X = pd.DataFrame({'x1': x1 - x1.mean(),\n",
    "                      'x2': x2 - x2.mean()\n",
    "                     })\n",
    "    # identity matrix\n",
    "    Id = np.diag(np.ones(n))\n",
    "    # QR factorization\n",
    "    Q = np.linalg.qr(X[['x1']])[0]\n",
    "    P = Q @ Q.T\n",
    "    x2o = (Id - P) @ X[['x2']]\n",
    "    Xc2 = pd.DataFrame({'x1': X['x1'], 'x2': x2o['x2']})\n",
    "    # divide by l2 norm\n",
    "    Y = Xc2 / np.sqrt(np.sum(np.square(Xc2), axis=0))\n",
    "    return Y['x2'] + (1/np.tan(theta)) * Y['x1'] + mean_return\n",
    "\n",
    "shorts = target_corr(real_data_df['S&P'].values, 0.9, -0.05)\n",
    "print(\"mean return %.04f\" % shorts.mean())\n",
    "print(\"vol %.04f\" % shorts.std())\n",
    "np.corrcoef(shorts, real_data_df['S&P'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = real_data_df.copy()\n",
    "df['shorts'] = shorts.to_list()\n",
    "labels = ['S&P', 'Small Caps', 'Real Estate', 'T-Bills', 'T-Notes', 'Gold', 'Baa Corps', 'shorts']\n",
    "\n",
    "df[['S&P', 'shorts']].plot.line();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance matrix\n",
    "Sigma = np.cov(df.transpose())\n",
    "# number of assets\n",
    "\n",
    "n = Sigma.shape[0]\n",
    "# average returns\n",
    "mu = df.mean().values\n",
    "# asset STDs\n",
    "asset_vols = np.sqrt(Sigma.diagonal())\n",
    "# variable to optimize over - portfolio weights\n",
    "w = cp.Variable(n)\n",
    "\n",
    "# objectives to optimize\n",
    "# portfolio return\n",
    "ret = mu.T @ w \n",
    "# volatility\n",
    "vol = cp.quad_form(w, Sigma)\n",
    "\n",
    "z = pd.DataFrame([mu, asset_vols], columns=labels)\n",
    "z['rows'] = ['real return', 'vol']\n",
    "z.set_index('rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = sns.diverging_palette(10, 220, sep=80, n=50)\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".02f\", cmap=my_cmap);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve max return portfolio (corner solution)\n",
    "# update constraints for leverage scenario\n",
    "# sum of weights == 1 as before, net long 100%\n",
    "# remove w >= 0 constraint\n",
    "# new constraint on gross exposure <= 1.5, otherwise optimal weights are unbounded (go infinity long S&P, infinity short stonks)\n",
    "\n",
    "prob = cp.Problem(cp.Maximize(ret), \n",
    "                  [cp.norm1(w) <= 1.5,  # gross exposure \n",
    "                   cp.sum(w) == 1]      # net exposure\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "maxretvol = vol.value\n",
    "maxret = ret.value\n",
    "print(\"Max return portfolio weights (return=%.4f, vol=%.4f)\" % (maxret, maxretvol))\n",
    "pd.DataFrame([wts], columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve min vol portfolio (other corner solution)\n",
    "prob = cp.Problem(cp.Minimize(vol),\n",
    "                  [cp.norm1(w) <= 1.5,\n",
    "                   cp.sum(w) == 1]\n",
    "                 )\n",
    "prob.solve()\n",
    "wts = [float('%0.4f' % v) for v in w.value]\n",
    "\n",
    "minvol = vol.value\n",
    "minvolret = ret.value\n",
    "print(\"Min vol portfolio weights (return=%.4f, vol=%.4f)\" % (minvolret, minvol))\n",
    "pd.DataFrame([wts], columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# solve points in between\n",
    "# maximize return subject to volatility constraints between minimum volatility and max return volatility\n",
    "\n",
    "# specify a Parameter variable instead of creating new Problem at each iteration\n",
    "# this allows the solver to reuse previous work\n",
    "vol_limit = cp.Parameter(nonneg=True)\n",
    "\n",
    "prob = cp.Problem(cp.Maximize(ret), \n",
    "                  [cp.norm1(w) <= 1.5,\n",
    "                   cp.sum(w) == 1,\n",
    "                   vol <= vol_limit]\n",
    "                 )\n",
    "\n",
    "# define function so we can solve many in parallel\n",
    "def solve_vl(vl_val):\n",
    "    vol_limit.value = vl_val\n",
    "    result = prob.solve()\n",
    "    return (ret.value, np.sqrt(vol.value), w.value)\n",
    "\n",
    "# number of points on the frontier\n",
    "NPOINTS = 200\n",
    "vl_vals = np.linspace(np.sqrt(minvol), np.sqrt(maxretvol), NPOINTS)\n",
    "vl_vals = np.square(vl_vals)\n",
    "# vol constraint is in variance space, take square root of minvol and maxvol, linspace, square values)\n",
    "\n",
    "# iterate in-process\n",
    "results_dict = {}\n",
    "for vl_val in vl_vals:\n",
    "    # print(datetime.strftime(datetime.now(), \"%H:%M:%S\"), vl_val)\n",
    "    results_dict[vl_val] = solve_vl(vl_val)\n",
    "    \n",
    "# parallel implementation\n",
    "# NPROCESSES = 8\n",
    "# pool = Pool(processes = NPROCESSES)\n",
    "# result_values = pool.map(solve_vl, vl_vals)\n",
    "# results_dict = dict(zip(vl_vals, result_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df = pd.DataFrame(enumerate(results_dict.keys()))\n",
    "ret_df.columns=['i', 'vol']\n",
    "ret_df['return'] = [results_dict[v][0] for v in ret_df['vol']]\n",
    "ret_df['std'] = [results_dict[v][1] for v in ret_df['vol']]\n",
    "for i, colname in enumerate(labels):\n",
    "    ret_df[colname]=[results_dict[v][2][i] for v in ret_df['vol']]\n",
    "# ret_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sharpe_return, max_sharpe_std, avg_ret_mean, avg_ret_std = plot_efrontier(ret_df, df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_map(ret_df, labels=df.columns, startyear=df.index[0], endyear=df.index[-1], max_sharpe_return=max_sharpe_return, avg_ret_mean=avg_ret_mean, ylim=1.5)\n",
    "# these are absolute values for gross exposure , not net exposure. \n",
    "# left looks weird because < 1.5 gross exposure but also additional Treasury shorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net exposure always 100%\n",
    "ret_df[labels].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gross exposure varies\n",
    "ret_df[labels].abs().sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min risk portfolio actually shorts t-bills and t-notes \n",
    "ret_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative approaches\n",
    "\n",
    "- If we thought returns were really IID log normally distributed and covariance stationary, then this optimization approach whould generate an optimal forward-looking efficient frontier.\n",
    "- But we can see that there are regime changes so covariance stationarity isn't a very good assumption, nor is log normality.\n",
    "- Since we might be overfitting to a particular past regime, we might want to regularize, which is to say back off from the model's maximum in a systematic way towards a more diversified solution robust to regime changes, while still near optimal.\n",
    "- A few regularization approaches with thanks to [Roman Rubsamen and PortfolioOptimizer.io](https://portfoliooptimizer.io):\n",
    "    - [Near optimal portfolios](https://portfoliooptimizer.io/blog/mean-variance-optimization-in-practice-well-diversified-near-efficient-portfolios/) One approach is, first find the highest Sharpe portfolio. Then we can say, find the lowest risk portfolio with no more than e.g. a 0.05 drop in Sharpe ratio. Since this portfolio is more diversified, i.e. most diversified within 0.05 of maximum Sharpe, it should be more robust out-of-sample.\n",
    "    - [Subset resampled portfolios](https://portfoliooptimizer.io/blog/mean-variance-optimization-in-practice-subset-resampling-based-efficient-portfolios/), Suppose we have 6 assets, do 6 optimizations, dropping one asset each time, then average all the portfolios. Similar to random forest, an ensemble of slightly weakened models performs better out of sample than a single overfitted model.\n",
    "    - [Michaud resampling](https://docs.portfoliooptimizer.io/index.html#post-/portfolios/analysis/mean-variance/efficient-frontier/resampling-based) and [MCOS](https://github.com/enjine-com/mcos/tree/master). Do Monte Carlo simulations where we perturb the return forecasts and covariances randomly each time, and average all the resulting portfolios. \n",
    "    - [Hierarchical Risk Parity](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2708678) and related methods like Nested Cluster Optimization. Create a tree of assets clustered by similarity. Then starting at the bottom at each non-leaf create a risk parity portfolio of assets under the node, and recursively climb the tree to get a global portfolio. If you create an portfolio of t-bonds and notes and bills and corporates (NCO) and MBS and munis, we will get some of each, even if one is dominated under MV optimization. Same if you do US stocks and various international markets with different market caps and geographies. Then as you combine clusters, all asset classes are represented, whereas a global optimization might omit some assets. Vanilla HRP actually ignores returns and correlations, uses only variances, and creates a minimum risk portfolio assuming no covariance at each level.\n",
    "    - Or the naive approach above, where we just average over the entire efficient frontier. One could do something more systematic and test how much regularization works best out of sample. One could average over e.g. all the efficient frontiers for 10- or 20-year periods or something.\n",
    "- To sum up\n",
    "    - If you do know your future returns and covariances, the efficient frontier and MV optimization give an optimal answer\n",
    "    - And also in that case, if you can invest or borrow at the risk free rate (or close), you can do better with the Sharpe-optimal portfolio plus leverage or deleverage vs. moving left or right along the frontier due to its convexity properties\n",
    "    - However, you probably don't have really good forecasts, and even really smart and sophisticated investors get burned by leverage. For instance before the financial crisis, Citibank and Harvard both decided they should be taking more risk and leverage and got burned.\n",
    "    - Also there is the ['equity premium puzzle'](https://en.wikipedia.org/wiki/Equity_premium_puzzle). Historically taking equity risk has been well compensated. Conventional wisdom is that if you have a long term horizon and are able to weather swings in equity returns without selling low, then you should hold more equity than suggested by these Sharpe-optimal portfolios. See this [paper](https://www.nber.org/system/files/working_papers/w10483/w10483.pdf), do a web search for 'equity premium puzzle' or have a conversation with your favorite advanced AI about what various eminent professors of finance have said about it.\n",
    "    - For these reasons you probably want to back off from MV optimal portfolios in the direction of more diversification and robustness to regime change, and if you prefer more risk and return than the Sharpe portfolio, you may want to move along the curve instead of using leverage.\n",
    "    - If you can perturb forecasts in a way that models past regime changes well, Michaud resampling seems like a sound approach. I would need to do more work to understand that and parameterize it though.\n",
    "    - A simple average of portfolios along the frontier is directionally similar and not massively suboptimal, and easier to explain and offers more diversification. It's near optimal without the 'near'. So it could be considered a 'not-too-far-from-optimal asset allocation for midwits', or a naive base case. It may be the simplest, dumbest asset allocation that might possibly work, as a starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical risk parity\n",
    "\n",
    "- Using the correlation matrix we can cluster assets into similar clusters and put equal risk into each cluster\n",
    "  - Use agglomerative clustering to make a binary tree of assets\n",
    "  - Start at individual assets, combine the 2 most correlated into a cluster\n",
    "  - Continue iteratively combining the most correlated assets or clusters into bigger clusters, until you arrive at the root.\n",
    "  - Now starting at the leaf nodes, allocate each asset to its parent cluster in inverse proportion to variance, and continue iteratively up the tree.\n",
    "  - A heuristic way to create a low variance portfolio, but unlike the minimum-variance optimization it's not very sensitive to small changes in correlations and doesn't use a return forecast. Kind of a poor man's minimum variance portfolio.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1928-present\n",
    "df = real_data_df.loc[1928:].copy()\n",
    "labels = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show returns and vols \n",
    "Sigma = np.cov(df.transpose())\n",
    "# number of assets\n",
    "\n",
    "n = Sigma.shape[0]\n",
    "# average returns\n",
    "mu = df.mean().values\n",
    "# asset STDs\n",
    "asset_vols = np.sqrt(Sigma.diagonal())\n",
    "# variable to optimize over - portfolio weights\n",
    "w = cp.Variable(n)\n",
    "\n",
    "# objectives to optimize\n",
    "# portfolio return\n",
    "ret = mu.T @ w \n",
    "# volatility\n",
    "vol = cp.quad_form(w, Sigma)\n",
    "\n",
    "z = pd.DataFrame([mu, asset_vols], columns=labels)\n",
    "z['rows'] = ['real return', 'vol']\n",
    "z.set_index('rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Riskfolio portfolio object\n",
    "port = rp.HCPortfolio(returns=df)\n",
    "\n",
    "# Estimate optimal portfolio:\n",
    "\n",
    "model='HRP' \n",
    "codependence = 'pearson' # Correlation matrix used to group assets in clusters\n",
    "rm = 'MV' # Risk measure used, this time will be variance\n",
    "rf = 0 # Risk free rate\n",
    "linkage_method = 'single' # Linkage method used to build clusters\n",
    "max_k = 10 # Max number of clusters used in two difference gap statistic, only for HERC model\n",
    "leaf_order = True # Consider optimal order of leafs in dendrogram\n",
    "\n",
    "w = port.optimization(model=model,\n",
    "                      codependence=codependence,\n",
    "                      rm=rm,\n",
    "                      rf=rf,\n",
    "                      linkage=linkage_method,\n",
    "                      max_k=max_k,\n",
    "                      leaf_order=leaf_order)\n",
    "display(w.T)\n",
    "# concentrated but should be less concentrated than minimum variance portfolio which is:\n",
    "# \tS&P\tReal Estate\tT-Bills\tT-Notes\tGold\tBaa Corps\n",
    "# \t0.001076\t0.361574\t0.599897\t-0.0\t0.037453\t-0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested Cluster Optimization\n",
    "df = df.loc[1928:]\n",
    "labels = df.columns\n",
    "\n",
    "# Building the portfolio object\n",
    "port = rp.HCPortfolio(returns=df)\n",
    "\n",
    "# Estimate optimal portfolio:\n",
    "\n",
    "model='NCO' \n",
    "codependence = 'pearson' # Correlation matrix used to group assets in clusters\n",
    "rm = 'MV' # Risk measure used, this time will be variance\n",
    "rf = 0 # Risk free rate\n",
    "linkage_method = 'single' # Linkage method used to build clusters\n",
    "max_k = 10 # Max number of clusters used in two difference gap statistic, only for HERC model\n",
    "leaf_order = True # Consider optimal order of leafs in dendrogram\n",
    "\n",
    "w = port.optimization(model=model,\n",
    "                      codependence=codependence,\n",
    "                      rm=rm,\n",
    "                      rf=rf,\n",
    "                      linkage=linkage_method,\n",
    "                      max_k=max_k,\n",
    "                      leaf_order=leaf_order)\n",
    "display(w.T)  # more concentrated than HRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute HRP ourselves instead using riskfolio module\n",
    "\n",
    "def cov_to_corr(cov):\n",
    "    \"\"\"Convert covariance matrix to correlation matrix.\"\"\"\n",
    "    if not np.allclose(cov, cov.T):  # check symmetry\n",
    "        raise ValueError(\"Covariance matrix is not symmetric\")\n",
    "    sd = np.sqrt(np.diag(cov)) # covariance to SD\n",
    "    corr = cov / np.outer(sd, sd) # scale cov[i,j] by dividing by sd[i[*sd[j] to get correlation\n",
    "    # fix some numerical precision errors\n",
    "    # Use isclose to fix values close to 0\n",
    "    corr[np.isclose(corr, 0, atol=1e-9)] = 0\n",
    "    # ensure not > 1 or < -1\n",
    "    corr = np.clip(corr, -1, 1)  \n",
    "    return corr\n",
    "\n",
    "def get_correlation_dist(corr):\n",
    "    \"\"\"Convert covariance matrix to a correlation distance matrix.\"\"\"\n",
    "    dist = np.sqrt((1 - corr)/2)\n",
    "    dist = np.clip(dist, 0, None)      # fix numerical precision errors, dist never <0\n",
    "    np.fill_diagonal(dist.values, 0)\n",
    "    return dist\n",
    "\n",
    "def show_dendrogram(linkage, labels):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    dendrogram(linkage,\n",
    "               labels=labels,\n",
    "               orientation='top',\n",
    "               leaf_rotation=90,\n",
    "               leaf_font_size=10,\n",
    "               show_contracted=True\n",
    "              )\n",
    "    plt.title('Hierarchical Clustering of Asset Returns')\n",
    "    plt.xlabel('Assets')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate covariance matrix\n",
    "covariance_matrix = df.cov()\n",
    "# use Ledoit-Wolf robust covariance when more than 30 or so assets (cols = 30% of rows)\n",
    "# Ledoit-Wolf is a cross between full covariance matrix and the one that would be generated by a single-factor beta model\n",
    "# lw = LedoitWolf()\n",
    "# covariance_matrix = pd.DataFrame(lw.fit(df).covariance_, index=df.columns, columns=df.columns)\n",
    "\n",
    "correlation_matrix = cov_to_corr(covariance_matrix)\n",
    "distance_matrix = get_correlation_dist(correlation_matrix)\n",
    "# 'ward' method tends to create balanced clusters. original de Prado paper used single linkage\n",
    "# riskfolio uses ward\n",
    "# need to study this more, you want to balance max between-cluster distance and min within-cluster distance\n",
    "# Ward's method: minimizes distance within clusters (need to understand how it avoids single element clusters with 0 distance)\n",
    "# Single linkage (nearest point): minimum distance between any 2 members\n",
    "# Complete linkage (furthest point): maximum distance between any members (ok but once you put farthest points in distinct clusters how to you determine the others)\n",
    "# Average linkage: average distance between all members\n",
    "\n",
    "link = linkage(squareform(distance_matrix), method='ward')\n",
    "show_dendrogram(link, df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves_list(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[leaves_list(link)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_indices = leaves_list(link)\n",
    "# reorder everything\n",
    "correlation_matrix = correlation_matrix.iloc[ordered_indices, ordered_indices]\n",
    "covariance_matrix = covariance_matrix.iloc[ordered_indices, ordered_indices]\n",
    "distance_matrix = distance_matrix.iloc[ordered_indices, ordered_indices]\n",
    "df = df.iloc[:, ordered_indices].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = sns.diverging_palette(10, 220, sep=80, n=50)\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".02f\", cmap=my_cmap);\n",
    "# can't help feeling corps should come before T-Notes, seems more correlated with S&P so WTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example linkage array\n",
    "# [[1, 3, 0.3, 2],    # Merge observations 1 & 3, distance=0.3, size=2 - create observation 4\n",
    "#  [0, 4, 0.4, 3],    # Merge observations 0 & 4 (from step 1), distance=0.4, size=2 - create observation 5\n",
    "#  [2, 5, 0.6, 4]]    # Merge observations 2 & 5 (from step 2), distance=0.6, size=3\n",
    "\n",
    "# create a linkage array based on bisection\n",
    "# basically throw out the tree and just make one based on the sort order that falls out of the linkage array\n",
    "# this is how it was done in the original paper but frankly, we already have a tree that contains good info \n",
    "# so I don't really understand why we wouldn't just use that.\n",
    "# writing this code so we can go either way\n",
    "# this will merge corps and T-notes, then stocks and gold, then merge both clusters.\n",
    "\n",
    "def recursive_bisection(corr_matrix):\n",
    "    \"\"\"\n",
    "    Perform recursive bisection on correlation matrix and return linkage-like structure\n",
    "    Returns array with format similar to scipy.cluster.hierarchy.linkage:\n",
    "    [[cluster1, cluster2, distance, size], ...]\n",
    "    Really doesn't matter that the values are correlations, just uses the row/column order\n",
    "    \"\"\"\n",
    "    n_children = len(corr_matrix)\n",
    "    # Initialize with individual assets as clusters\n",
    "    # Will store our linkage-like information\n",
    "    bisection_links = []\n",
    "    last_index = 0\n",
    "    \n",
    "    def split_cluster(cluster_indices):\n",
    "        n_children = len(cluster_indices)\n",
    "        if n <= 1:\n",
    "            return cluster_indices\n",
    "        else:\n",
    "            split = n_children // 2\n",
    "            return cluster_indices[split:], cluster_indices[:split]\n",
    "    \n",
    "    # Start with all assets in one cluster\n",
    "    queue = [list(range(n_children))]\n",
    "    \n",
    "    while queue:\n",
    "        current_cluster = queue.pop(0)\n",
    "        if len(current_cluster) <= 1:\n",
    "            continue\n",
    "            \n",
    "        # Split cluster\n",
    "        cluster1, cluster2 = split_cluster(current_cluster)\n",
    "        \n",
    "        # Calculate distance (1 - avg correlation between clusters)\n",
    "        avg_corr = corr_matrix.iloc[cluster1, cluster2].mean().mean()\n",
    "        distance = 1 - avg_corr\n",
    "        \n",
    "        # Add to linkage-like structure\n",
    "        if len(cluster1) > 1:\n",
    "            cluster1_index = last_index - 1\n",
    "            last_index = cluster1_index\n",
    "        elif len(cluster1) == 1:\n",
    "            cluster1_index = cluster1[0]\n",
    "        else:\n",
    "            print(\"error in bisection\")\n",
    "            \n",
    "        if len(cluster2) > 1:\n",
    "            cluster2_index = last_index - 1\n",
    "            last_index = cluster2_index\n",
    "        elif len(cluster2) == 1:\n",
    "            cluster2_index = cluster2[0]\n",
    "        else:\n",
    "            print(\"error in bisection\")\n",
    "\n",
    "        bisection_links.append([\n",
    "            cluster1_index, cluster2_index,  # use minimum index from each cluster\n",
    "            distance,\n",
    "            len(cluster1) + len(cluster2)\n",
    "        ])\n",
    "        \n",
    "        # Add new clusters to queue if they're larger than 1\n",
    "        if len(cluster1) > 1:\n",
    "            queue.append(cluster1)\n",
    "        if len(cluster2) > 1:\n",
    "            queue.append(cluster2)\n",
    "\n",
    "    n_new_clusters = -last_index\n",
    "    highest_cluster_number = n_children + n_new_clusters - 1\n",
    "    retarray = []\n",
    "    for a in bisection_links:\n",
    "        cluster1_index = highest_cluster_number + a[0] + 1 if a[0] < 0 else a[0]\n",
    "        cluster2_index = highest_cluster_number + a[1] + 1 if a[1] < 0 else a[1]\n",
    "        retarray.append([\n",
    "            cluster1_index, cluster2_index,  # use minimum index from each cluster\n",
    "            a[2],\n",
    "            a[3]\n",
    "        ])\n",
    "    return list(reversed(retarray))\n",
    "\n",
    "bisection_links = recursive_bisection(correlation_matrix)\n",
    "\n",
    "bisection_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 12x12 DataFrame of random numbers between -1 and 1\n",
    "# random_df = pd.DataFrame(np.random.uniform(-1, 1, size=(12, 12)))\n",
    "\n",
    "def recursive_bisection(corr_matrix):\n",
    "    \"\"\"\n",
    "    Perform recursive bisection on ordered correlation matrix and return linkage-like structure.\n",
    "    Returns array with format similar to scipy.cluster.hierarchy.linkage:\n",
    "    [[cluster1, cluster2, distance, size], ...]\n",
    "    \"\"\"\n",
    "    n = len(corr_matrix)\n",
    "    bisection_links = []\n",
    "    next_cluster_id = n\n",
    "    \n",
    "    def split_cluster(indices):\n",
    "        \"\"\"Split cluster into two parts based on current order\"\"\"\n",
    "        mid = len(indices) // 2\n",
    "        return indices[:mid], indices[mid:]\n",
    "    \n",
    "    def process_cluster(indices):\n",
    "        \"\"\"Recursively process clusters and build linkage structure\"\"\"\n",
    "        if len(indices) <= 1:\n",
    "            return indices[0]\n",
    "            \n",
    "        # Split cluster\n",
    "        left_indices, right_indices = split_cluster(indices)\n",
    "        \n",
    "        # Process sub-clusters recursively\n",
    "        left_id = process_cluster(left_indices) if len(left_indices) > 0 else None\n",
    "        right_id = process_cluster(right_indices) if len(right_indices) > 0 else None\n",
    "        \n",
    "        # Calculate distance (1 - avg correlation between clusters)\n",
    "        avg_corr = corr_matrix.iloc[left_indices, right_indices].mean().mean()\n",
    "        distance = 1 - avg_corr\n",
    "        \n",
    "        # Add to linkage structure\n",
    "        nonlocal next_cluster_id\n",
    "        bisection_links.append([\n",
    "            left_id,\n",
    "            right_id,\n",
    "            distance,\n",
    "            len(left_indices) + len(right_indices)\n",
    "        ])\n",
    "        \n",
    "        cluster_id = next_cluster_id\n",
    "        next_cluster_id += 1\n",
    "        return cluster_id\n",
    "    \n",
    "    # Start recursive process with all indices\n",
    "    process_cluster(list(range(n)))\n",
    "    \n",
    "    return np.array(bisection_links)\n",
    "\n",
    "# Example usage:\n",
    "bisection_links = recursive_bisection(correlation_matrix)  # use the already ordered correlation matrix\n",
    "print(\"Bisection linkage structure:\")\n",
    "print(bisection_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hrp_weights(link, cov_matrix):\n",
    "    \"\"\"\n",
    "    Calculate HRP portfolio weights using linkage structure\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    link : numpy.ndarray\n",
    "        Linkage matrix with shape (n-1, 4) where n is number of assets\n",
    "    cov_matrix : pandas.DataFrame\n",
    "        Covariance matrix of asset returns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series\n",
    "        Portfolio weights indexed by asset names\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(cov_matrix, pd.DataFrame):\n",
    "        raise TypeError(\"cov_matrix must be a pandas DataFrame\")\n",
    "    if not np.allclose(cov_matrix, cov_matrix.T):\n",
    "        raise ValueError(\"cov_matrix must be symmetric\")\n",
    "    if link.shape[1] != 4:\n",
    "        raise ValueError(\"link must have 4 columns (cluster1, cluster2, distance, size)\")\n",
    "    \n",
    "    def cluster_variance(cluster_assets):\n",
    "        \"\"\"Calculate variance of a cluster using equal weights\"\"\"\n",
    "        cluster_cov = cov_matrix.iloc[cluster_assets, cluster_assets]\n",
    "        w = np.ones(len(cluster_assets)) / len(cluster_assets)\n",
    "        variance = np.transpose(w) @ cluster_cov @ w\n",
    "        return max(0, variance) # numerical stability check, force non-neg\n",
    "\n",
    "    n = len(cov_matrix)\n",
    "    weights = np.ones(n)\n",
    "    \n",
    "    # Initialize clusters dictionary: at start, each asset is in its own cluster\n",
    "    clusters = {i: [i] for i in range(n)}\n",
    "    \n",
    "    # Process each merge from the linkage\n",
    "    for i, row in enumerate(link):\n",
    "        cluster1_idx = int(row[0])\n",
    "        cluster2_idx = int(row[1])\n",
    "        new_cluster_idx = n + i\n",
    "        \n",
    "        # Get assets in each cluster\n",
    "        cluster1_assets = clusters[cluster1_idx]\n",
    "        cluster2_assets = clusters[cluster2_idx]\n",
    "        \n",
    "        # Calculate cluster variances\n",
    "        var1 = cluster_variance(cluster1_assets)\n",
    "        var2 = cluster_variance(cluster2_assets)\n",
    "        \n",
    "        # Calculate weights\n",
    "        alpha = 1 - (var1 / (var1 + var2))\n",
    "        \n",
    "        # Update weights\n",
    "        for idx in cluster1_assets:\n",
    "            weights[idx] *= alpha\n",
    "        for idx in cluster2_assets:\n",
    "            weights[idx] *= (1 - alpha)\n",
    "        \n",
    "        # Store the merged cluster\n",
    "        clusters[new_cluster_idx] = cluster1_assets + cluster2_assets\n",
    "        \n",
    "        # Verify cluster size matches linkage info\n",
    "        if len(clusters[new_cluster_idx]) != int(row[3]):\n",
    "            raise ValueError(f\"Cluster size mismatch at step {i}\")    \n",
    "            \n",
    "    # Normalize weights\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    return pd.Series(weights, index=cov_matrix.index)\n",
    "\n",
    "calculate_hrp_weights(bisection_links, covariance_matrix)\n",
    "\n",
    "# doesn't match riskfolio\n",
    "# S&P\tT-Notes\tGold\tBaa Corps\n",
    "# weights\t0.110835\t0.311695\t0.167578\t0.409892\n",
    "\n",
    "# matches pretty close if leaf_order = False\n",
    "# \tS&P\tT-Notes\tGold\tBaa Corps\n",
    "# weights\t0.182752\t0.225961\t0.222854\t0.368433\n",
    "\n",
    "# TODO : look up how riskfolio enforces leaf order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo this without bisecting, just use the tree created in the agglomerative clustering step.\n",
    "# reorder df\n",
    "\n",
    "# Calculate covariance matrix\n",
    "covariance_matrix = df.cov()\n",
    "# use Ledoit-Wolf robust covariance when more than 30 or so assets (cols = 30% of rows)\n",
    "# Ledoit-Wolf is a cross between full covariance matrix and the one that would be generated by a single-factor beta model\n",
    "# lw = LedoitWolf()\n",
    "# covariance_matrix = pd.DataFrame(lw.fit(df).covariance_, index=df.columns, columns=df.columns)\n",
    "\n",
    "correlation_matrix = cov_to_corr(covariance_matrix)\n",
    "distance_matrix = get_correlation_dist(correlation_matrix)\n",
    "# 'ward' method tends to create balanced clusters. original paper used single linkage\n",
    "# riskfolio uses ward\n",
    "# need to study this more, you want to balance max between-cluster distance and min within-cluster distance\n",
    "# Ward's method: minimizes distance within clusters (need to understand how it avoids single element clusters with 0 distance)\n",
    "# Single linkage (nearest point): minimum distance between any 2 members\n",
    "# Complete linkage (furthest point): maximum distance between any members (ok but once you put farthest points in distinct clusters how to you determine the others)\n",
    "# Average linkage: average distance between all members\n",
    "\n",
    "link = linkage(squareform(distance_matrix), method='ward')\n",
    "show_dendrogram(link, df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrp_weights = calculate_hrp_weights(link, covariance_matrix)\n",
    "hrp_weights\n",
    "\n",
    "# more gold because merge is 2,3,4 instead fo 2,2,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_table(df, wts):\n",
    "    cols = wts.index.tolist()\n",
    "    returns = df[cols] @ wts\n",
    "    mu = returns.mean()\n",
    "    sd = returns.std()\n",
    "    sharpe = mu/sd\n",
    "    print(f\"Mean return: {100*mu:3.2f}%\")\n",
    "    print(f\"Vol:         {100*sd:3.2f}%\")\n",
    "    print(f\"Sharpe:      {mu/sd:3.3f}%\")\n",
    "\n",
    "    return mu, sd, sharpe\n",
    "\n",
    "ret_table(df, hrp_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Sharpe using portfoliooptimizer.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load API key for portfoliooptimizer.io\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1928-present\n",
    "df = real_data_df.loc[1928:].copy()\n",
    "labels = df.columns\n",
    "n_years, n_assets = df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = df.mean().to_list()\n",
    "covmatrix = df.cov().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint and headers\n",
    "BASEURL = \"https://api.portfoliooptimizer.io/v1\"\n",
    "ENDPOINT = \"portfolio/optimization/maximum-sharpe-ratio\"\n",
    "url = f\"{BASEURL}/{ENDPOINT}\"\n",
    "print(url)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f'Bearer {os.getenv(\"PORTFOLIO_OPT_APIKEY\")})'\n",
    "}\n",
    "\n",
    "# Payload\n",
    "data = {\n",
    "    \"assets\": n_assets,\n",
    "    \"assetsReturns\": mu,\n",
    "    \"assetsCovarianceMatrix\": covmatrix.tolist(),\n",
    "    \"riskFreeRate\": 0\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Check and print the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Response data:\", response.json())\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = json.loads(response.text)['assetsWeights']\n",
    "wts_df = pd.DataFrame({'Asset': df.columns.to_list(), 'Weight': wts}).set_index(\"Asset\")\n",
    "wts_df\n",
    "\n",
    "# pretty close to above, note that we slice into 200 variances so discretization will impact it a little \n",
    "\n",
    "# Max Sharpe Portfolio:\n",
    "# Real Return: 3.63%\n",
    "# SD:          6.16%\n",
    "# T-Bills: 0.0%\n",
    "# Real Estate: 39.3%\n",
    "# T-Notes: 0.0%\n",
    "# Baa Corps: 33.3%\n",
    "# Gold: 11.9%\n",
    "# S&P: 13.5%\n",
    "# Small Caps: 2.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_table(df, wts_df[\"Weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Near optimal diversified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Near optimal portfolios](https://portfoliooptimizer.io/blog/mean-variance-optimization-in-practice-well-diversified-near-efficient-portfolios/) \n",
    "\n",
    "First find the highest Sharpe portfolio. Then, find the lowest risk portfolio with no more than e.g. a 0.05 drop in Sharpe ratio. Since this portfolio is more diversified, i.e. most diversified within 0.05 of maximum Sharpe, it should be more robust out-of-sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1928-present\n",
    "df = real_data_df.loc[1928:].copy()\n",
    "labels = df.columns\n",
    "n_years, n_assets = df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = df.mean().to_list()\n",
    "covmatrix = df.cov().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs\n",
    "# https://docs.portfoliooptimizer.io/index.html#post-/portfolio/optimization/maximum-sharpe-ratio/diversified\n",
    "\n",
    "# API endpoint and headers\n",
    "BASEURL = \"https://api.portfoliooptimizer.io/v1\"\n",
    "ENDPOINT = \"portfolio/optimization/maximum-sharpe-ratio/diversified\"\n",
    "url = f\"{BASEURL}/{ENDPOINT}\"\n",
    "print(url)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f'Bearer {os.getenv(\"PORTFOLIO_OPT_APIKEY\")})'\n",
    "}\n",
    "\n",
    "# Payload\n",
    "# I'm surprised there is no parameter for how 'near' is considered 'near optimal'\n",
    "\n",
    "data = {\n",
    "    \"assets\": n_assets,\n",
    "    \"assetsReturns\": mu,\n",
    "    \"assetsCovarianceMatrix\": covmatrix.tolist(),\n",
    "    \"riskFreeRate\": 0\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "time.sleep(1) # can max out rate limit when run end to end\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Check and print the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Response data:\", response.json())\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = json.loads(response.text)['assetsWeights']\n",
    "wts_df = pd.DataFrame({'Asset': df.columns.to_list(), 'Weight': wts}).set_index(\"Asset\")\n",
    "wts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_table(df, wts_df[\"Weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Resampling\n",
    "\n",
    "[Subset resampled portfolios](https://portfoliooptimizer.io/blog/mean-variance-optimization-in-practice-subset-resampling-based-efficient-portfolios/), Suppose we have 6 assets, do 6 optimizations, dropping one asset each time, then average all the portfolios. Similar to random forest, an ensemble of slightly weakened models is better than a single overfitted model.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1928-present\n",
    "df = real_data_df.loc[1928:].copy()\n",
    "df[\"TIPS\"] = 0\n",
    "labels = df.columns\n",
    "n_years, n_assets = df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = df.mean().to_list()\n",
    "covmatrix = df.cov().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint and headers\n",
    "BASEURL = \"https://api.portfoliooptimizer.io/v1\"\n",
    "ENDPOINT = \"portfolio/optimization/maximum-sharpe-ratio/subset-resampling-based\"\n",
    "url = f\"{BASEURL}/{ENDPOINT}\"\n",
    "print(url)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f'Bearer {os.getenv(\"PORTFOLIO_OPT_APIKEY\")})'\n",
    "}\n",
    "\n",
    "# Payload\n",
    "data = {\n",
    "    \"assets\": n_assets,\n",
    "    \"assetsReturns\": mu,\n",
    "    \"assetsCovarianceMatrix\": covmatrix.tolist(),\n",
    "    \"riskFreeRate\": 0\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "time.sleep(1) # can max out rate limit when run end to end\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Check and print the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Response data:\", response.json())\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = json.loads(response.text)['assetsWeights']\n",
    "wts_df = pd.DataFrame({'Asset': df.columns.to_list(), 'Weight': wts}).set_index(\"Asset\")\n",
    "wts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_table(df, wts_df[\"Weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michaud Resampling\n",
    "\n",
    "\n",
    "[Michaud resampling](https://docs.portfoliooptimizer.io/index.html#post-/portfolios/analysis/mean-variance/efficient-frontier/resampling-based) and [MCOS](https://github.com/enjine-com/mcos/tree/master). Do Monte Carlo simulations where we perturb the return forecasts and covariances randomly each time, and average all the resulting portfolios. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1928-present\n",
    "df = real_data_df.loc[1928:].copy()\n",
    "df[\"TIPS\"] = 0\n",
    "labels = df.columns\n",
    "n_years, n_assets = df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = df.mean().to_list()\n",
    "covmatrix = df.cov().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make intervals for mus , maybe should be a multiple of sds, not sure\n",
    "mu_adjust = [m/3 for m in mu]\n",
    "mu_adjust = [max(0.01, m) for m in mu_adjust] # at least 1% uncertainty interval or 30%\n",
    "mu_intervals = list(zip([mu[i] - mu_adjust[i] for i in range(len(mu))], \n",
    "                        [mu[i] + mu_adjust[i] for i in range(len(mu))]))\n",
    "mu_intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint and headers\n",
    "BASEURL = \"https://api.portfoliooptimizer.io/v1\"\n",
    "ENDPOINT = \"portfolios/optimization/maximum-sharpe-ratio/resampling-based\"\n",
    "url = f\"{BASEURL}/{ENDPOINT}\"\n",
    "print(url)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f'Bearer {os.getenv(\"PORTFOLIO_OPT_APIKEY\")})'\n",
    "}\n",
    "\n",
    "# Payload\n",
    "data = {\n",
    "    \"assets\": n_assets,\n",
    "    \"assetsReturns\": mu,\n",
    "    \"assetsReturnsUncertaintyIntervals\": mu_intervals,\n",
    "    \"assetsCovarianceMatrix\": covmatrix.tolist(),\n",
    "    \"riskFreeRate\": 0,\n",
    "#     \"assetsCorrelationMatrixUncertaintyLevel\": 0.25,\n",
    "#     \"portfolios\": 100\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "time.sleep(1) # can max out rate limit when run end to end\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Check and print the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Response data:\", response.json())\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = json.loads(response.text)['assetsWeights']\n",
    "wts_df = pd.DataFrame({'Asset': df.columns.to_list(), 'Weight': wts}).set_index(\"Asset\")\n",
    "wts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_table(df, wts_df[\"Weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors\n",
    "\n",
    "Optimal portfolio after decomposing returns into risk factors with PCA/SVD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do this experiment with a synthetic portfolio\n",
    "# number of stocksa\n",
    "n = 1000 \n",
    "# historical mean returns for each stock\n",
    "mu = np.random.normal(0.1, 0.2, n)\n",
    "\n",
    "# number of factors\n",
    "m = 10\n",
    "\n",
    "# factor covariance matrix - random symmetrical matrix\n",
    "SigmaFactor = np.random.randn(m, m)/4\n",
    "SigmaFactor = SigmaFactor.T @ SigmaFactor\n",
    "\n",
    "# factor loadings, determine volatility and covariances between stocks\n",
    "F = np.random.randn(n, m)\n",
    "# idiosyncratic risk of each stock\n",
    "D = np.diag(np.random.uniform(0, 0.9, size=n))\n",
    "\n",
    "count, bins, ignored = plt.hist(mu, 100, density=True, align='mid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = cp.Variable(n)         # what we solve for: weight for each stock\n",
    "ret = mu.T @ w             # solve for weights that maximize portfolio return\n",
    "f = F.T @ w                # portfolio factor loading\n",
    "Lmax = cp.Parameter()      # leverage constraint\n",
    "# portfolio volatility: factor risk + idiosyncratic risk\n",
    "risk = cp.quad_form(f, SigmaFactor) + cp.quad_form(w, D)\n",
    "prob = cp.Problem(cp.Maximize(ret), \n",
    "                  [cp.sum(w) == 1, \n",
    "                   cp.norm(w, 1) <= Lmax])\n",
    "\n",
    "# Solve the factor model problem.\n",
    "Lmax.value = 2\n",
    "prob.solve(verbose=True)\n",
    "\n",
    "maxretvol = risk.value\n",
    "maxret = ret.value\n",
    "print(\"Max return portfolio (return=%.4f, vol=%.4f)\" % (maxret, maxretvol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve min vol portfolio (other corner solution)\n",
    "\n",
    "prob = cp.Problem(cp.Minimize(risk),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   cp.norm(w, 1) <= Lmax])\n",
    "prob.solve(solver=cp.OSQP)\n",
    "\n",
    "minvol = risk.value\n",
    "minvolret = ret.value\n",
    "print(\"Min vol portfolio (return=%.4f, risk=%.4f)\" % (minvolret, minvol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# solve points in between\n",
    "# maximize return subject to volatility constraints between minimum volatility and max return volatility\n",
    "# might have to run a couple of times to get a solution\n",
    "\n",
    "# specify a Parameter variable instead of creating new Problem at each iteration\n",
    "# this allows the solver to reuse previous work\n",
    "vol_limit = cp.Parameter(nonneg=True)\n",
    "\n",
    "prob = cp.Problem(cp.Maximize(ret),\n",
    "                  [cp.sum(w) == 1, \n",
    "                   cp.norm(w, 1) <= Lmax,\n",
    "                   risk <= vol_limit]\n",
    "                 )\n",
    "\n",
    "# define function so we can solve many in parallel\n",
    "def solve_vl(vl_val):\n",
    "    vol_limit.value = vl_val\n",
    "    result = prob.solve(verbose=False)\n",
    "    return (ret.value, np.sqrt(risk.value), w.value)\n",
    "\n",
    "# number of points on the frontier\n",
    "NPOINTS = 200\n",
    "vl_vals = np.linspace(np.sqrt(minvol), np.sqrt(maxretvol), NPOINTS)\n",
    "vl_vals = np.square(vl_vals)\n",
    "# vol constraint is in variance space, take square root of minvol and maxvol, linspace, square values)\n",
    "\n",
    "# iterate in-process\n",
    "results_dict = {}\n",
    "for vl_val in vl_vals:\n",
    "    # print(datetime.strftime(datetime.now(), \"%H:%M:%S\"), vl_val)\n",
    "    results_dict[vl_val] = solve_vl(vl_val)\n",
    "    \n",
    "# parallel implementation\n",
    "# NPROCESSES = 8\n",
    "# pool = Pool(processes = NPROCESSES)\n",
    "# result_values = pool.map(solve_vl, vl_vals)\n",
    "# results_dict = dict(zip(vl_vals, result_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df = pd.DataFrame(enumerate(results_dict.keys()))\n",
    "ret_df.columns=['i', 'var']\n",
    "ret_df['return'] = [results_dict[v][0] for v in ret_df['var']]\n",
    "ret_df['std'] = [results_dict[v][1] for v in ret_df['var']]\n",
    "# ret_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot frontier\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "#plt.scatter(asset_vols, mu)\n",
    "\n",
    "x = ret_df['std']\n",
    "y = ret_df['return']\n",
    "plt.xlabel(\"Standard Deviation of Returns\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Risk vs. Return\")\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x,y);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or compute factors with historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to openbb with email and password\n",
    "obb.account.login(email=os.environ['OPENBB_USER'], password=os.environ['OPENBB_PW'], remember_me=True)\n",
    "\n",
    "# probably a way to get S&P components from OpenBB but this didn't give tickers, have to map lei to ticker, might need a paid provider sub\n",
    "# response = obb.etf.holdings(symbol='VOO', provider='sec')\n",
    "# response.results[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tickers from Wikipedia\n",
    "ticker_list = ['A',\n",
    " 'AAPL',\n",
    " 'ABBV',\n",
    " 'ABNB',\n",
    " 'ABT',\n",
    " 'ACGL',\n",
    " 'ACN',\n",
    " 'ADBE',\n",
    " 'ADI',\n",
    " 'ADM',\n",
    " 'ADP',\n",
    " 'ADSK',\n",
    " 'AEE',\n",
    " 'AEP',\n",
    " 'AES',\n",
    " 'AFL',\n",
    " 'AIG',\n",
    " 'AIZ',\n",
    " 'AJG',\n",
    " 'AKAM',\n",
    " 'ALB',\n",
    " 'ALGN',\n",
    " 'ALL',\n",
    " 'ALLE',\n",
    " 'AMAT',\n",
    " 'AMCR',\n",
    " 'AMD',\n",
    " 'AME',\n",
    " 'AMGN',\n",
    " 'AMP',\n",
    " 'AMT',\n",
    " 'AMZN',\n",
    " 'ANET',\n",
    " 'ANSS',\n",
    " 'AON',\n",
    " 'AOS',\n",
    " 'APA',\n",
    " 'APD',\n",
    " 'APH',\n",
    " 'APO',\n",
    " 'APTV',\n",
    " 'ARE',\n",
    " 'ATO',\n",
    " 'AVB',\n",
    " 'AVGO',\n",
    " 'AVY',\n",
    " 'AWK',\n",
    " 'AXON',\n",
    " 'AXP',\n",
    " 'AZO',\n",
    " 'BA',\n",
    " 'BAC',\n",
    " 'BALL',\n",
    " 'BAX',\n",
    " 'BBY',\n",
    " 'BDX',\n",
    " 'BEN',\n",
    " 'BF-B',\n",
    " 'BG',\n",
    " 'BIIB',\n",
    " 'BK',\n",
    " 'BKNG',\n",
    " 'BKR',\n",
    " 'BLDR',\n",
    " 'BLK',\n",
    " 'BMY',\n",
    " 'BR',\n",
    " 'BRK-B',\n",
    " 'BRO',\n",
    " 'BSX',\n",
    " 'BWA',\n",
    " 'BX',\n",
    " 'BXP',\n",
    " 'C',\n",
    " 'CAG',\n",
    " 'CAH',\n",
    " 'CARR',\n",
    " 'CAT',\n",
    " 'CB',\n",
    " 'CBOE',\n",
    " 'CBRE',\n",
    " 'CCI',\n",
    " 'CCL',\n",
    " 'CDNS',\n",
    " 'CDW',\n",
    " 'CE',\n",
    " 'CEG',\n",
    " 'CF',\n",
    " 'CFG',\n",
    " 'CHD',\n",
    " 'CHRW',\n",
    " 'CHTR',\n",
    " 'CI',\n",
    " 'CINF',\n",
    " 'CL',\n",
    " 'CLX',\n",
    " 'CMCSA',\n",
    " 'CME',\n",
    " 'CMG',\n",
    " 'CMI',\n",
    " 'CMS',\n",
    " 'CNC',\n",
    " 'CNP',\n",
    " 'COF',\n",
    " 'COO',\n",
    " 'COP',\n",
    " 'COR',\n",
    " 'COST',\n",
    " 'CPAY',\n",
    " 'CPB',\n",
    " 'CPRT',\n",
    " 'CPT',\n",
    " 'CRL',\n",
    " 'CRM',\n",
    " 'CRWD',\n",
    " 'CSCO',\n",
    " 'CSGP',\n",
    " 'CSX',\n",
    " 'CTAS',\n",
    " 'CTRA',\n",
    " 'CTSH',\n",
    " 'CTVA',\n",
    " 'CVS',\n",
    " 'CVX',\n",
    " 'CZR',\n",
    " 'D',\n",
    " 'DAL',\n",
    " 'DAY',\n",
    " 'DD',\n",
    " 'DE',\n",
    " 'DECK',\n",
    " 'DELL',\n",
    " 'DFS',\n",
    " 'DG',\n",
    " 'DGX',\n",
    " 'DHI',\n",
    " 'DHR',\n",
    " 'DIS',\n",
    " 'DLR',\n",
    " 'DLTR',\n",
    " 'DOC',\n",
    " 'DOV',\n",
    " 'DOW',\n",
    " 'DPZ',\n",
    " 'DRI',\n",
    " 'DTE',\n",
    " 'DUK',\n",
    " 'DVA',\n",
    " 'DVN',\n",
    " 'DXCM',\n",
    " 'EA',\n",
    " 'EBAY',\n",
    " 'ECL',\n",
    " 'ED',\n",
    " 'EFX',\n",
    " 'EG',\n",
    " 'EIX',\n",
    " 'EL',\n",
    " 'ELV',\n",
    " 'EMN',\n",
    " 'EMR',\n",
    " 'ENPH',\n",
    " 'EOG',\n",
    " 'EPAM',\n",
    " 'EQIX',\n",
    " 'EQR',\n",
    " 'EQT',\n",
    " 'ERIE',\n",
    " 'ES',\n",
    " 'ESS',\n",
    " 'ETN',\n",
    " 'ETR',\n",
    " 'EVRG',\n",
    " 'EW',\n",
    " 'EXC',\n",
    " 'EXPD',\n",
    " 'EXPE',\n",
    " 'EXR',\n",
    " 'F',\n",
    " 'FANG',\n",
    " 'FAST',\n",
    " 'FCX',\n",
    " 'FDS',\n",
    " 'FDX',\n",
    " 'FE',\n",
    " 'FFIV',\n",
    " 'FI',\n",
    " 'FICO',\n",
    " 'FIS',\n",
    " 'FITB',\n",
    " 'FMC',\n",
    " 'FOX',\n",
    " 'FOXA',\n",
    " 'FRT',\n",
    " 'FSLR',\n",
    " 'FTNT',\n",
    " 'FTV',\n",
    " 'GD',\n",
    " 'GDDY',\n",
    " 'GE',\n",
    " 'GEHC',\n",
    " 'GEN',\n",
    " 'GEV',\n",
    " 'GILD',\n",
    " 'GIS',\n",
    " 'GL',\n",
    " 'GLW',\n",
    " 'GM',\n",
    " 'GNRC',\n",
    " 'GOOG',\n",
    " 'GOOGL',\n",
    " 'GPC',\n",
    " 'GPN',\n",
    " 'GRMN',\n",
    " 'GS',\n",
    " 'GWW',\n",
    " 'HAL',\n",
    " 'HAS',\n",
    " 'HBAN',\n",
    " 'HCA',\n",
    " 'HD',\n",
    " 'HES',\n",
    " 'HIG',\n",
    " 'HII',\n",
    " 'HLT',\n",
    " 'HOLX',\n",
    " 'HON',\n",
    " 'HPE',\n",
    " 'HPQ',\n",
    " 'HRL',\n",
    " 'HSIC',\n",
    " 'HST',\n",
    " 'HSY',\n",
    " 'HUBB',\n",
    " 'HUM',\n",
    " 'HWM',\n",
    " 'IBM',\n",
    " 'ICE',\n",
    " 'IDXX',\n",
    " 'IEX',\n",
    " 'IFF',\n",
    " 'INCY',\n",
    " 'INTC',\n",
    " 'INTU',\n",
    " 'INVH',\n",
    " 'IP',\n",
    " 'IPG',\n",
    " 'IQV',\n",
    " 'IR',\n",
    " 'IRM',\n",
    " 'ISRG',\n",
    " 'IT',\n",
    " 'ITW',\n",
    " 'IVZ',\n",
    " 'J',\n",
    " 'JBHT',\n",
    " 'JBL',\n",
    " 'JCI',\n",
    " 'JKHY',\n",
    " 'JNJ',\n",
    " 'JNPR',\n",
    " 'JPM',\n",
    " 'K',\n",
    " 'KDP',\n",
    " 'KEY',\n",
    " 'KEYS',\n",
    " 'KHC',\n",
    " 'KIM',\n",
    " 'KKR',\n",
    " 'KLAC',\n",
    " 'KMB',\n",
    " 'KMI',\n",
    " 'KMX',\n",
    " 'KO',\n",
    " 'KR',\n",
    " 'KVUE',\n",
    " 'L',\n",
    " 'LDOS',\n",
    " 'LEN',\n",
    " 'LH',\n",
    " 'LHX',\n",
    " 'LII',\n",
    " 'LIN',\n",
    " 'LKQ',\n",
    " 'LLY',\n",
    " 'LMT',\n",
    " 'LNT',\n",
    " 'LOW',\n",
    " 'LRCX',\n",
    " 'LULU',\n",
    " 'LUV',\n",
    " 'LVS',\n",
    " 'LW',\n",
    " 'LYB',\n",
    " 'LYV',\n",
    " 'MA',\n",
    " 'MAA',\n",
    " 'MAR',\n",
    " 'MAS',\n",
    " 'MCD',\n",
    " 'MCHP',\n",
    " 'MCK',\n",
    " 'MCO',\n",
    " 'MDLZ',\n",
    " 'MDT',\n",
    " 'MET',\n",
    " 'META',\n",
    " 'MGM',\n",
    " 'MHK',\n",
    " 'MKC',\n",
    " 'MKTX',\n",
    " 'MLM',\n",
    " 'MMC',\n",
    " 'MMM',\n",
    " 'MNST',\n",
    " 'MO',\n",
    " 'MOH',\n",
    " 'MOS',\n",
    " 'MPC',\n",
    " 'MPWR',\n",
    " 'MRK',\n",
    " 'MRNA',\n",
    " 'MS',\n",
    " 'MSCI',\n",
    " 'MSFT',\n",
    " 'MSI',\n",
    " 'MTB',\n",
    " 'MTCH',\n",
    " 'MTD',\n",
    " 'MU',\n",
    " 'NCLH',\n",
    " 'NDAQ',\n",
    " 'NDSN',\n",
    " 'NEE',\n",
    " 'NEM',\n",
    " 'NFLX',\n",
    " 'NI',\n",
    " 'NKE',\n",
    " 'NOC',\n",
    " 'NOW',\n",
    " 'NRG',\n",
    " 'NSC',\n",
    " 'NTAP',\n",
    " 'NTRS',\n",
    " 'NUE',\n",
    " 'NVDA',\n",
    " 'NVR',\n",
    " 'NWS',\n",
    " 'NWSA',\n",
    " 'NXPI',\n",
    " 'O',\n",
    " 'ODFL',\n",
    " 'OKE',\n",
    " 'OMC',\n",
    " 'ON',\n",
    " 'ORCL',\n",
    " 'ORLY',\n",
    " 'OTIS',\n",
    " 'OXY',\n",
    " 'PANW',\n",
    " 'PARA',\n",
    " 'PAYC',\n",
    " 'PAYX',\n",
    " 'PCAR',\n",
    " 'PCG',\n",
    " 'PEG',\n",
    " 'PEP',\n",
    " 'PFE',\n",
    " 'PFG',\n",
    " 'PG',\n",
    " 'PGR',\n",
    " 'PH',\n",
    " 'PHM',\n",
    " 'PKG',\n",
    " 'PLD',\n",
    " 'PLTR',\n",
    " 'PM',\n",
    " 'PNC',\n",
    " 'PNR',\n",
    " 'PNW',\n",
    " 'PODD',\n",
    " 'POOL',\n",
    " 'PPG',\n",
    " 'PPL',\n",
    " 'PRU',\n",
    " 'PSA',\n",
    " 'PSX',\n",
    " 'PTC',\n",
    " 'PWR',\n",
    " 'PYPL',\n",
    " 'QCOM',\n",
    " 'RCL',\n",
    " 'REG',\n",
    " 'REGN',\n",
    " 'RF',\n",
    " 'RJF',\n",
    " 'RL',\n",
    " 'RMD',\n",
    " 'ROK',\n",
    " 'ROL',\n",
    " 'ROP',\n",
    " 'ROST',\n",
    " 'RSG',\n",
    " 'RTX',\n",
    " 'RVTY',\n",
    " 'SBAC',\n",
    " 'SBUX',\n",
    " 'SCHW',\n",
    " 'SHW',\n",
    " 'SJM',\n",
    " 'SLB',\n",
    " 'SMCI',\n",
    " 'SNA',\n",
    " 'SNPS',\n",
    " 'SO',\n",
    " 'SOLV',\n",
    " 'SPG',\n",
    " 'SPGI',\n",
    " 'SRE',\n",
    " 'STE',\n",
    " 'STLD',\n",
    " 'STT',\n",
    " 'STX',\n",
    " 'STZ',\n",
    " 'SW',\n",
    " 'SWK',\n",
    " 'SWKS',\n",
    " 'SYF',\n",
    " 'SYK',\n",
    " 'SYY',\n",
    " 'T',\n",
    " 'TAP',\n",
    " 'TDG',\n",
    " 'TDY',\n",
    " 'TECH',\n",
    " 'TEL',\n",
    " 'TER',\n",
    " 'TFC',\n",
    " 'TFX',\n",
    " 'TGT',\n",
    " 'TJX',\n",
    " 'TMO',\n",
    " 'TMUS',\n",
    " 'TPL',\n",
    " 'TPR',\n",
    " 'TRGP',\n",
    " 'TRMB',\n",
    " 'TROW',\n",
    " 'TRV',\n",
    " 'TSCO',\n",
    " 'TSLA',\n",
    " 'TSN',\n",
    " 'TT',\n",
    " 'TTWO',\n",
    " 'TXN',\n",
    " 'TXT',\n",
    " 'TYL',\n",
    " 'UAL',\n",
    " 'UBER',\n",
    " 'UDR',\n",
    " 'UHS',\n",
    " 'ULTA',\n",
    " 'UNH',\n",
    " 'UNP',\n",
    " 'UPS',\n",
    " 'URI',\n",
    " 'USB',\n",
    " 'V',\n",
    " 'VICI',\n",
    " 'VLO',\n",
    " 'VLTO',\n",
    " 'VMC',\n",
    " 'VRSK',\n",
    " 'VRSN',\n",
    " 'VRTX',\n",
    " 'VST',\n",
    " 'VTR',\n",
    " 'VTRS',\n",
    " 'VZ',\n",
    " 'WAB',\n",
    " 'WAT',\n",
    " 'WBA',\n",
    " 'WBD',\n",
    " 'WDAY',\n",
    " 'WDC',\n",
    " 'WEC',\n",
    " 'WELL',\n",
    " 'WFC',\n",
    " 'WM',\n",
    " 'WMB',\n",
    " 'WMT',\n",
    " 'WRB',\n",
    " 'WST',\n",
    " 'WTW',\n",
    " 'WY',\n",
    " 'WYNN',\n",
    " 'XEL',\n",
    " 'XOM',\n",
    " 'XYL',\n",
    " 'YUM',\n",
    " 'ZBH',\n",
    " 'ZBRA',\n",
    " 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download historical returns from Yahoo Finance\n",
    "todays_date = datetime.today()\n",
    "start_date = datetime(year=todays_date.year-10, month=todays_date.month, day=todays_date.day)\n",
    "start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "symbol = \"BF-B\"\n",
    "df = obb.equity.price.historical(symbol = symbol, \n",
    "                                 start_date = start_date, \n",
    "                                 provider=\"yfinance\",\n",
    "                                 adjustment='splits_and_dividends').to_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all tickers\n",
    "df = obb.equity.price.historical(symbol = ticker_list, \n",
    "                                 start_date = start_date, \n",
    "                                 provider=\"yfinance\",\n",
    "                                 adjustment='splits_and_dividends').to_df()\n",
    "# too big to display\n",
    "# df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"symbol\", \"close\"]].copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix index\n",
    "df = df.reset_index()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df.pivot(index=\"date\", columns=[\"symbol\"], values=\"close\")\n",
    "df_pivot.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "df_pivot.to_pickle(\"df_pivot.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change from daily to monthly returns\n",
    "dfx = df_pivot \\\n",
    "    .resample('ME') \\\n",
    "    .last() \\\n",
    "    .pct_change() \\\n",
    "    .dropna(axis=0, how='all') \\\n",
    "    .dropna(axis=1)      # drop columns with missing data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(dfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart eigenvalues\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Create elbow plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), \n",
    "         np.cumsum(explained_variance_ratio), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Elbow Plot of PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scree plot with individual variance explained\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), \n",
    "         explained_variance_ratio, 'bo-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot of PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.fit_transform(dfx)\n",
    "reconstructed = pca.inverse_transform(components)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "difference = np.abs(dfx - reconstructed)\n",
    "reconstruction_error = np.mean(difference)\n",
    "\n",
    "print(\"Reconstruction Error (Mean Absolute Difference):\")\n",
    "print(reconstruction_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = np.dot(components, pca.components_) + pca.mean_\n",
    "reconstructed_df = pd.DataFrame(reconstructed_data, columns=dfx.columns)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "reconstruction_error = np.mean(np.abs(dfx.reset_index(drop=True) - reconstructed_df))\n",
    "print(\"Reconstruction Error (Mean Absolute Difference):\", reconstruction_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct but with 20 columns\n",
    "components.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_ .shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use e.g. 20 factors that explain ~ 70% of variation\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  \n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    index=dfx.columns\n",
    ")\n",
    "loadings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest left as an exercise for the reader for now, may revisit later\n",
    "# compute efficient frontier of portfolios of top 20 factors\n",
    "# compute max sharpe portfolio \n",
    "# back out individual stock weights\n",
    "# compute backward looking performance, vol, sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio_optimization",
   "language": "python",
   "name": "py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
